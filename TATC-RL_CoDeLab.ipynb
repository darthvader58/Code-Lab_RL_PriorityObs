{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashwatraj/Documents/GitHub/Code-Lab_RL_PriorityObs/tatcenv/lib/python3.13/site-packages/pyogrio/raw.py:198: RuntimeWarning: driver GeoJSON does not support open option DRIVER\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       time satellite instrument    swath_width  valid_obs  \\\n",
      "0 2024-05-22 00:00:00+00:00      Test       Test  278640.704057       True   \n",
      "1 2024-05-22 00:02:00+00:00      Test       Test  277041.710508       True   \n",
      "2 2024-05-22 00:04:00+00:00      Test       Test  275419.732317       True   \n",
      "3 2024-05-22 00:06:00+00:00      Test       Test  273866.283658       True   \n",
      "4 2024-05-22 00:08:00+00:00      Test       Test  272466.738302       True   \n",
      "\n",
      "   solar_hour time_range  month    lat_sat     lon_sat  cnprcp_mean  \\\n",
      "0    9.006320    morning      5 -38.142798  134.265144          0.0   \n",
      "1    9.495089    morning      5 -33.073314  141.096599          0.0   \n",
      "2    9.929164    morning      5 -27.614694  147.107611          0.0   \n",
      "3   10.321410    morning      5 -21.873562  152.491185          0.0   \n",
      "4   10.683133    morning      5 -15.932572  157.416882          0.0   \n",
      "\n",
      "                                            geometry  time_step  \n",
      "0  POLYGON Z ((135.51668 -38.1428 0, 135.51065 -3...          0  \n",
      "1  POLYGON Z ((142.34095 -33.07331 0, 142.33496 -...          1  \n",
      "2  POLYGON Z ((148.34468 -27.61469 0, 148.33872 -...          2  \n",
      "3  POLYGON Z ((153.72128 -21.87356 0, 153.71535 -...          3  \n",
      "4  POLYGON Z ((158.64069 -15.93257 0, 158.63479 -...          4  \n",
      "Index(['time', 'satellite', 'instrument', 'swath_width', 'valid_obs',\n",
      "       'solar_hour', 'time_range', 'month', 'lat_sat', 'lon_sat',\n",
      "       'cnprcp_mean', 'geometry', 'time_step'],\n",
      "      dtype='object')\n",
      "94480\n",
      "3630\n"
     ]
    }
   ],
   "source": [
    "file_path = 'zip://clustered_data_4months.zip!clustered_data_4months (2).geojson'\n",
    "data = gpd.read_file(file_path, driver=\"GeoJSON\")\n",
    "\n",
    "# Convert 'time' to datetime and sort by time\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data = data.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# Create time_step as an ordinal index\n",
    "data['time_step'] = data.index\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "print(len(data))\n",
    "print(len(data[data['cnprcp_mean']>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file('110m_cultural.zip', layer='ne_110m_admin_0_boundary_lines_land')\n",
    "geometry = [Point(xy) for xy in zip(data['lon_sat'], data['lat_sat'])]\n",
    "geo_full = gpd.GeoDataFrame(data, geometry=geometry)\n",
    "geo_full['ground_track'] = geo_full.apply(lambda row: 0 if world.contains(row.geometry).any() else 1, axis=1)\n",
    "data['ground_track'] = geo_full['ground_track']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lat_rad'] = np.radians(data['lat_sat'])\n",
    "data['lon_rad'] = np.radians(data['lon_sat'])\n",
    "data['x'] = np.cos(data['lat_rad']) * np.cos(data['lon_rad'])\n",
    "data['y'] = np.cos(data['lat_rad']) * np.sin(data['lon_rad'])\n",
    "data['z'] = np.sin(data['lat_rad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "time_range_encoded = encoder.fit_transform(data[['time_range']])\n",
    "time_range_feature_names = encoder.get_feature_names_out(['time_range'])\n",
    "time_range_df = pd.DataFrame(time_range_encoded, columns=time_range_feature_names, index=data.index)\n",
    "data = pd.concat([data, time_range_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cols_to_scale = ['x', 'y', 'z', 'cnprcp_mean']\n",
    "scaled_values = scaler.fit_transform(data[cols_to_scale])\n",
    "data['x_norm'] = scaled_values[:, 0]\n",
    "data['y_norm'] = scaled_values[:, 1]\n",
    "data['z_norm'] = scaled_values[:, 2]\n",
    "data['cnprcp_norm'] = scaled_values[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orbits found: 1448\n"
     ]
    }
   ],
   "source": [
    "def compute_orbit_ids(df, lon_threshold=358):\n",
    "    \n",
    "    df = df.copy()\n",
    "    orbit_id = 0\n",
    "    orbit_ids = [orbit_id]\n",
    "    prev_lon = df.iloc[0]['lon_sat']\n",
    "    for idx in range(1, len(df)):\n",
    "        curr_lon = df.iloc[idx]['lon_sat']\n",
    "        # Handle wrap-around: e.g., near 180 and -180 are close\n",
    "        diff = abs(curr_lon - prev_lon)\n",
    "        if diff > lon_threshold:\n",
    "            orbit_id += 1\n",
    "        orbit_ids.append(orbit_id)\n",
    "        prev_lon = curr_lon\n",
    "    df['orbit_id'] = orbit_ids\n",
    "    return df\n",
    "\n",
    "data = compute_orbit_ids(data, lon_threshold=300)\n",
    "\n",
    "print(\"Number of orbits found:\", data['orbit_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(SatelliteEnv, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.n_steps = len(self.df)\n",
    "        \n",
    "\n",
    "        # Action space: 0 = don't act, 1 = act.\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # One-hot encoded time_range columns.\n",
    "        self.one_hot_cols = [col for col in df.columns if col.startswith('time_range_')]\n",
    "        n_one_hot = len(self.one_hot_cols)\n",
    "\n",
    "        self.obs_dim = 4 + n_one_hot\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(self.obs_dim,), dtype=np.float32)\n",
    "        self._index = 0\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self._index = 0\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self._compute_reward(action)\n",
    "        self._index += 1\n",
    "        done = (self._index >= self.n_steps - 1)\n",
    "        obs = self._get_obs() if not done else np.zeros(self.obs_dim, dtype=np.float32)\n",
    "        terminated = done  # assuming episode termination\n",
    "        truncated = False  # assuming no truncation\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        row = self.df.iloc[self._index]\n",
    "        # states : x_norm, y_norm, z_norm, ground_track.\n",
    "        cont_features = np.array([\n",
    "            row['x_norm'],\n",
    "            row['y_norm'],\n",
    "            row['z_norm'],\n",
    "            float(row['ground_track'])\n",
    "        ], dtype=np.float32)\n",
    "        # One-hot encoded time_range features:\n",
    "        one_hot_features = row[self.one_hot_cols].values.astype(np.float32)\n",
    "        # Concatenate the encoded vector and state features to form the full state.\n",
    "        obs = np.concatenate([cont_features, one_hot_features])\n",
    "        return obs\n",
    "    \n",
    "    # Define the Reward Function\n",
    "    # correct decision*intensity\n",
    "    def _compute_reward(self, action):\n",
    "        row = self.df.iloc[self._index]\n",
    "        cnprcp_mean = row['cnprcp_mean']\n",
    "        scale = 10000  # scaling factor to amplify small precipitation values\n",
    "        if action == 1:\n",
    "            if cnprcp_mean > 0:\n",
    "                # Correct detection: give a base reward of 1 plus bonus proportional to intensity.\n",
    "                reward = 1 + (cnprcp_mean * scale)\n",
    "            else:\n",
    "                # False positive: a small penalty for acting when there's no precipitation.\n",
    "                reward = -0.1\n",
    "        else:  # action == 0\n",
    "            if cnprcp_mean > 0:\n",
    "                reward = -0.001 #scale the negative penalty according to the intensity of precipitation\n",
    "            else:\n",
    "                reward = 0\n",
    "        return reward\n",
    "    \n",
    "    #visualize reward distribution for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For RandomForest model\n",
    "features = data[['x_norm', 'y_norm', 'z_norm', 'ground_track', 'orbit_id']]\n",
    "target = (data['cnprcp_mean'] > 0).astype(int)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1500, random_state=42, criterion='log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Precision: 0.24\n",
      "RandomForest - Recall: 0.05\n",
      "RandomForest - F1: 0.08 \n",
      "\n",
      "RandomForest - Precision: 0.26\n",
      "RandomForest - Recall: 0.05\n",
      "RandomForest - F1: 0.09 \n",
      "\n",
      "RandomForest - Precision: 0.36\n",
      "RandomForest - Recall: 0.04\n",
      "RandomForest - F1: 0.08 \n",
      "\n",
      "RandomForest - Precision: 0.23\n",
      "RandomForest - Recall: 0.07\n",
      "RandomForest - F1: 0.10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_test_results_random_forest =[]\n",
    "all_rewards_rf = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(features):\n",
    "    train_features, test_features = features.iloc[train_idx], features.iloc[test_idx]\n",
    "    train_target, test_target = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "    # Train and Evaluate RF Classifier Model\n",
    "    rf_classifier.fit(train_features, train_target)\n",
    "    rf_predictions = rf_classifier.predict(test_features)\n",
    "\n",
    "    rf_precision = precision_score(test_target, rf_predictions, zero_division=0)\n",
    "    rf_recall = recall_score(test_target, rf_predictions, zero_division=0)\n",
    "    rf_f1 = f1_score(test_target, rf_predictions, zero_division=0)\n",
    "    print(f\"RandomForest - Precision: {rf_precision:.2f}\")\n",
    "    print(f\"RandomForest - Recall: {rf_recall:.2f}\") \n",
    "    print(f\"RandomForest - F1: {rf_f1:.2f} \\n\") \n",
    "\n",
    "    tmp_df = test_features.copy()\n",
    "    tmp_df['actual'] = test_target.values\n",
    "    tmp_df['predicted'] = rf_predictions\n",
    "\n",
    "    all_test_results_random_forest.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: F1_Score = 0.000, Precision_Score = 0.000, Recall_Score = 0.000, Total Reward = -0.814\n",
      "Fold 1: F1_Score = 0.000, Precision_Score = 0.000, Recall_Score = 0.000, Total Reward = -0.781\n",
      "Fold 2: F1_Score = 0.000, Precision_Score = 0.000, Recall_Score = 0.000, Total Reward = -0.759\n",
      "Fold 3: F1_Score = 0.000, Precision_Score = 0.000, Recall_Score = 0.000, Total Reward = -0.425\n",
      "\n",
      "Overall: Mean F1 = 0.000, Mean Cumulative Reward = -0.695\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "all_f1_scores_a2c = []\n",
    "all_precision_scores_a2c = []\n",
    "all_recall_scores_a2c = []\n",
    "all_rewards_a2c = []\n",
    "all_test_results_a2c  = []\n",
    "\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "    train_df = data.iloc[train_idx]\n",
    "    test_df = data.iloc[test_idx]\n",
    "\n",
    "    train_env = SatelliteEnv(train_df)\n",
    "    check_env(train_env, warn=True)\n",
    "\n",
    "\n",
    "    model = A2C(\"MlpPolicy\", train_env, verbose=0)\n",
    "    model.learn(total_timesteps=100000)  \n",
    "\n",
    "    # 4) Evaluate on the test split\n",
    "    test_env = SatelliteEnv(test_df)\n",
    "    obs, _ = test_env.reset()\n",
    "\n",
    "    done = False\n",
    "    total_reward_a2c = 0.0\n",
    "    predicted_actions_a2c = []\n",
    "    actual_labels_a2c = []\n",
    "    tmp_lat_a2c = []\n",
    "    tmp_lon_a2c = []\n",
    "    tmp_test_results_a2c = {'actual': [], 'predicted': [], 'lat': [], 'lon': []}\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        predicted_actions_a2c.append(action)\n",
    "        \n",
    "        # Get actual label from cnprcp_mean: 1 if > 0, else 0.\n",
    "        current_cprcp = test_env.df.iloc[test_env._index]['cnprcp_mean']\n",
    "        actual_label_a2c = 1 if current_cprcp > 0 else 0\n",
    "        actual_labels_a2c.append(int(actual_label_a2c))\n",
    "        tmp_lat_a2c.append(test_env.df.iloc[test_env._index]['lat_sat'])\n",
    "        tmp_lon_a2c.append(test_env.df.iloc[test_env._index]['lon_sat'])\n",
    "\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        total_reward_a2c += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    f1 = f1_score(actual_labels_a2c, predicted_actions_a2c, zero_division=0)\n",
    "    precision_a2c = precision_score(actual_labels_a2c, predicted_actions_a2c, zero_division=0)\n",
    "    recall_a2c = recall_score(actual_labels_a2c, predicted_actions_a2c, zero_division=0)\n",
    "    tmp_test_results_a2c['actual']+= actual_labels_a2c\n",
    "    tmp_test_results_a2c['predicted']+= predicted_actions_a2c\n",
    "    tmp_test_results_a2c['lat']+= tmp_lat_a2c\n",
    "    tmp_test_results_a2c['lon']+= tmp_lon_a2c\n",
    "    all_test_results_a2c.append(tmp_test_results_a2c)\n",
    "    all_f1_scores_a2c.append(f1)\n",
    "    all_precision_scores_a2c.append(precision_a2c)\n",
    "    all_recall_scores_a2c.append(recall_a2c)\n",
    "    all_rewards_a2c.append(total_reward_a2c)\n",
    "    print(f\"Fold {fold_idx}: F1_Score = {f1:.3f}, Precision_Score = {precision_a2c:.3f}, Recall_Score = {recall_a2c:.3f}, Total Reward = {total_reward_a2c:.3f}\")\n",
    "\n",
    "mean_f1_a2c = np.mean(all_f1_scores_a2c)\n",
    "mean_reward_a2c = np.mean(all_rewards_a2c)\n",
    "print(f\"\\nOverall: Mean F1 = {mean_f1_a2c:.3f}, Mean Cumulative Reward = {mean_reward_a2c:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: F1_Score = 0.209, Precision_Score = 0.121, Recall_Score = 0.794, Total Reward = 619.828\n",
      "Fold 1: F1_Score = 0.217, Precision_Score = 0.128, Recall_Score = 0.723, Total Reward = 556.741\n",
      "Fold 2: F1_Score = 0.214, Precision_Score = 0.129, Recall_Score = 0.623, Total Reward = 495.420\n",
      "Fold 3: F1_Score = 0.119, Precision_Score = 0.066, Recall_Score = 0.633, Total Reward = 104.565\n",
      "\n",
      "Overall: Mean F1 = 0.190, Mean Cumulative Reward = 444.139\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "all_f1_scores_dqn = []\n",
    "all_precision_scores_dqn = []\n",
    "all_recall_scores_dqn = []\n",
    "all_rewards_dqn = []\n",
    "all_test_results_dqn  = []\n",
    "\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "    train_df = data.iloc[train_idx]\n",
    "    test_df = data.iloc[test_idx]\n",
    "\n",
    "    train_env = SatelliteEnv(train_df)\n",
    "    check_env(train_env, warn=True)\n",
    "\n",
    "\n",
    "    model = DQN(\"MlpPolicy\", train_env, verbose=0)\n",
    "    model.learn(total_timesteps=100000)  \n",
    "\n",
    "    # 4) Evaluate on the test split\n",
    "    test_env = SatelliteEnv(test_df)\n",
    "    obs, _ = test_env.reset()\n",
    "\n",
    "    done = False\n",
    "    total_reward_dqn = 0.0\n",
    "    predicted_actions_dqn = []\n",
    "    actual_labels_dqn = []\n",
    "    tmp_lat_dqn = []\n",
    "    tmp_lon_dqn = []\n",
    "    tmp_test_results_dqn = {'actual': [], 'predicted': [], 'lat': [], 'lon': []}\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        predicted_actions_dqn.append(action)\n",
    "        \n",
    "        # Get actual label from cnprcp_mean: 1 if > 0, else 0.\n",
    "        current_cprcp = test_env.df.iloc[test_env._index]['cnprcp_mean']\n",
    "        actual_label_dqn = 1 if current_cprcp > 0 else 0\n",
    "        actual_labels_dqn.append(int(actual_label_dqn))\n",
    "        tmp_lat_dqn.append(test_env.df.iloc[test_env._index]['lat_sat'])\n",
    "        tmp_lon_dqn.append(test_env.df.iloc[test_env._index]['lon_sat'])\n",
    "\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        total_reward_dqn += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    f1 = f1_score(actual_labels_dqn, predicted_actions_dqn, zero_division=0)\n",
    "    precision_dqn = precision_score(actual_labels_dqn, predicted_actions_dqn, zero_division=0)\n",
    "    recall_dqn = recall_score(actual_labels_dqn, predicted_actions_dqn, zero_division=0)\n",
    "    tmp_test_results_dqn['actual']+= actual_labels_dqn\n",
    "    tmp_test_results_dqn['predicted']+= predicted_actions_dqn\n",
    "    tmp_test_results_dqn['lat']+= tmp_lat_dqn\n",
    "    tmp_test_results_dqn['lon']+= tmp_lon_dqn\n",
    "    all_test_results_dqn.append(tmp_test_results_dqn)\n",
    "    all_f1_scores_dqn.append(f1)\n",
    "    all_precision_scores_dqn.append(precision_dqn)\n",
    "    all_recall_scores_dqn.append(recall_dqn)\n",
    "    all_rewards_dqn.append(total_reward_dqn)\n",
    "    print(f\"Fold {fold_idx}: F1_Score = {f1:.3f}, Precision_Score = {precision_dqn:.3f}, Recall_Score = {recall_dqn:.3f}, Total Reward = {total_reward_dqn:.3f}\")\n",
    "\n",
    "mean_f1_dqn = np.mean(all_f1_scores_dqn)\n",
    "mean_reward_dqn = np.mean(all_rewards_dqn)\n",
    "print(f\"\\nOverall: Mean F1 = {mean_f1_dqn:.3f}, Mean Cumulative Reward = {mean_reward_dqn:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: F1_Score = 0.105, Precision_Score = 0.057, Recall_Score = 0.703, Total Reward = -32.173\n",
      "Fold 1: F1_Score = 0.117, Precision_Score = 0.063, Recall_Score = 0.740, Total Reward = 77.419\n",
      "Fold 2: F1_Score = 0.088, Precision_Score = 0.048, Recall_Score = 0.516, Total Reward = -100.857\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib import QRDQN \n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "all_f1_scores_qrdqn = []\n",
    "all_precision_scores_qrdqn = []\n",
    "all_recall_scores_qrdqn = []\n",
    "all_rewards_qrdqn = []\n",
    "all_test_results_qrdqn  = []\n",
    "\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "    train_df = data.iloc[train_idx]\n",
    "    test_df = data.iloc[test_idx]\n",
    "\n",
    "    train_env = SatelliteEnv(train_df)\n",
    "    check_env(train_env, warn=True)\n",
    "\n",
    "\n",
    "    model = QRDQN(\"MlpPolicy\", train_env, verbose=0)\n",
    "    model.learn(total_timesteps=100000)  \n",
    "\n",
    "    # 4) Evaluate on the test split\n",
    "    test_env = SatelliteEnv(test_df)\n",
    "    obs, _ = test_env.reset()\n",
    "\n",
    "    done = False\n",
    "    total_reward_qrdqn = 0.0\n",
    "    predicted_actions_qrdqn = []\n",
    "    actual_labels_qrdqn = []\n",
    "    tmp_lat_qrdqn = []\n",
    "    tmp_lon_qrdqn = []\n",
    "    tmp_test_results_qrdqn = {'actual': [], 'predicted': [], 'lat': [], 'lon': []}\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        predicted_actions_qrdqn.append(action)\n",
    "        \n",
    "        # Get actual label from cnprcp_mean: 1 if > 0, else 0.\n",
    "        current_cprcp = test_env.df.iloc[test_env._index]['cnprcp_mean']\n",
    "        actual_label_qrdqn = 1 if current_cprcp > 0 else 0\n",
    "        actual_labels_qrdqn.append(int(actual_label_qrdqn))\n",
    "        tmp_lat_qrdqn.append(test_env.df.iloc[test_env._index]['lat_sat'])\n",
    "        tmp_lon_qrdqn.append(test_env.df.iloc[test_env._index]['lon_sat'])\n",
    "\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        total_reward_qrdqn += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    f1 = f1_score(actual_labels_qrdqn, predicted_actions_qrdqn, zero_division=0)\n",
    "    precision_qrdqn = precision_score(actual_labels_qrdqn, predicted_actions_qrdqn, zero_division=0)\n",
    "    recall_qrdqn = recall_score(actual_labels_qrdqn, predicted_actions_qrdqn, zero_division=0)\n",
    "    tmp_test_results_qrdqn['actual']+= actual_labels_qrdqn\n",
    "    tmp_test_results_qrdqn['predicted']+= predicted_actions_qrdqn\n",
    "    tmp_test_results_qrdqn['lat']+= tmp_lat_qrdqn\n",
    "    tmp_test_results_qrdqn['lon']+= tmp_lon_qrdqn\n",
    "    all_test_results_dqn.append(tmp_test_results_qrdqn)\n",
    "    all_f1_scores_qrdqn.append(f1)\n",
    "    all_precision_scores_qrdqn.append(precision_qrdqn)\n",
    "    all_recall_scores_qrdqn.append(recall_qrdqn)\n",
    "    all_rewards_qrdqn.append(total_reward_qrdqn)\n",
    "    print(f\"Fold {fold_idx}: F1_Score = {f1:.3f}, Precision_Score = {precision_qrdqn:.3f}, Recall_Score = {recall_qrdqn:.3f}, Total Reward = {total_reward_qrdqn:.3f}\")\n",
    "\n",
    "mean_f1_qrdqn = np.mean(all_f1_scores_qrdqn)\n",
    "mean_reward_qrdqn = np.mean(all_rewards_qrdqn)\n",
    "print(f\"\\nOverall: Mean F1 = {mean_f1_qrdqn:.3f}, Mean Cumulative Reward = {mean_reward_qrdqn:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "all_actual_qrdqn = []\n",
    "all_predicted_qrdqn = []\n",
    "for result in all_test_results_qrdqn:\n",
    "    all_actual_qrdqn.extend(result[\"actual\"])\n",
    "    all_predicted_qrdqn.extend(result[\"predicted\"])\n",
    "\n",
    "fpr_qrdqn, tpr_qrdqn, thresholds = roc_curve(all_actual_qrdqn, all_predicted_qrdqn)\n",
    "roc_auc_qrdqn = roc_auc_score(all_actual_qrdqn, all_predicted_qrdqn)\n",
    "\n",
    "all_actual_dqn = []\n",
    "all_predicted_dqn = []\n",
    "for result in all_test_results_dqn:\n",
    "    all_actual_dqn.extend(result[\"actual\"])\n",
    "    all_predicted_dqn.extend(result[\"predicted\"])\n",
    "\n",
    "fpr_dqn, tpr_dqn, thresholds = roc_curve(all_actual_dqn, all_predicted_dqn)\n",
    "roc_auc_dqn = roc_auc_score(all_actual_dqn, all_predicted_dqn)\n",
    "\n",
    "all_actual_a2c = []\n",
    "all_predicted_a2c = []\n",
    "for result in all_test_results_a2c:\n",
    "    all_actual_a2c.extend(result[\"actual\"])\n",
    "    all_predicted_a2c.extend(result[\"predicted\"])\n",
    "\n",
    "fpr_a2c, tpr_a2c, thresholds = roc_curve(all_actual_a2c, all_predicted_a2c)\n",
    "roc_auc_a2c = roc_auc_score(all_actual_a2c, all_predicted_a2c)\n",
    "\n",
    "all_actual_rf = []\n",
    "all_predicted_rf = []\n",
    "for result in all_test_results_random_forest:\n",
    "    all_actual_rf.extend(result[\"actual\"])\n",
    "    all_predicted_rf.extend(result[\"predicted\"])\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds = roc_curve(all_actual_rf, all_predicted_rf)\n",
    "roc_auc_rf = roc_auc_score(all_actual_rf, all_predicted_rf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_qrdqn, tpr_qrdqn, label=f\"C51 ROC curve (AUC = {roc_auc_qrdqn:.2f})\", color='blue')\n",
    "plt.plot(fpr_dqn, tpr_dqn, label=f\"DQN ROC curve (AUC = {roc_auc_dqn:.2f})\", color='orange')\n",
    "plt.plot(fpr_a2c, tpr_a2c, label=f\"A2C ROC curve (AUC = {roc_auc_a2c:.2f})\", color='green')\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest ROC curve (AUC = {roc_auc_rf:.2f})\", color='red')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Different Models\")    \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8,6))\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.kdeplot(all_rewards_a2c, ax=ax, fill=False, color='blue', alpha=0.5, label = 'A2C')\n",
    "sns.kdeplot(all_rewards_dqn, ax=ax, fill=False, color='pink', alpha=0.5, label = 'DQN')\n",
    "sns.kdeplot(all_rewards_qrdqn, ax=ax, fill=False, color='green', alpha=0.5, label = 'QR-DQN')\n",
    "plt.xlabel(\"Cumulative Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Reward Distribution for different Models\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
